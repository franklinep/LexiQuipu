{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Vectorización de los Chunks Generados**\n",
    "\n",
    "En este notebook vamos a vectorizar (convertir en embeddings) a los chunks que hemos guardado en el archivo corpus.json generado por el script pipene_data.py, usaremos SentenceTransformer con un modelo pre-entrenado con varios idiomas incluido el español.\n",
    "\n",
    "Para entender mejor el objetivo de lo que haremos es ubicar cada chunk en un espacio n-dimensional de manera que el modelo pueda identificar la relación semantica que hay entre los chunks ¿Como asi? haremos un ejemplo sencillo:\n",
    "\n",
    "- Un chunk que habla de \"reposición por despido\" estará muy cerca de otro que mencione \"reincorporación de un trabajador\".\n",
    "- Un chunk que hable de \"régimen laboral privado\" estará lejos de uno que hable de \"derecho penal\".\n",
    "\n",
    "Al convertir estos chunks de textos a embeddings la computadora puede \"entender\" la similitud que tienen estos chunks a traves del angulo que hacen estos vectores en el espacio muldimensional. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el corpus\n",
    "with open(\"../src/corpus.json\", 'r', encoding='utf-8') as f:\n",
    "    corpus = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta primera muestra, vamos a trabajar con una muestra de 10 PDF's del total de nuestro corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cada muestra tenemos el nombre del archivo, los metadatos y los chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['archivo', 'metadatos', 'chunks'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cada metadatos hemos extraido el numero de casacion, el lugar, y la materia de la sentencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['n_casacion', 'lugar', 'materia'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]['metadatos'].keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cada chunk de una sentencia extrajimos información como la sumilla, vista, materia, desicion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CORTE SUPREMA DE JUSTICIA DE L...\n",
      "\n",
      "Sumilla. Es en ese contexto do...\n",
      "\n",
      "VISTA la causa número dieciséi...\n",
      "\n",
      "MATERIA DEL RECURSO \n",
      "Se trata ...\n",
      "\n",
      "CAUSAL DEL RECURSO \n",
      "El recurso...\n",
      "\n",
      "CONSIDERANDO \n",
      "Desarrollo del p...\n",
      "\n",
      "DECISIÓN \n",
      "Declararon FUNDADO e...\n"
     ]
    }
   ],
   "source": [
    "for chunk in corpus[0]['chunks']:\n",
    "    print(\"\\n\" +chunk[:30] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos un modelo preentrenado con multiples lenguajes incluso el español:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a iterar sobre cada uno de los documentos de nuestro corpus para generar los embedings, y luego añadimos el embedding a cada chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectorizando documentos: 100%|██████████| 10/10 [00:03<00:00,  3.18it/s]\n"
     ]
    }
   ],
   "source": [
    "for document in tqdm(corpus, desc=\"Vectorizando documentos\"):\n",
    "    \n",
    "    chunk_texts = [chunk for chunk in document['chunks']]\n",
    "    \n",
    "    embeddings = model.encode(chunk_texts, show_progress_bar=False)\n",
    "    \n",
    "    enriched_chunks = []\n",
    "    for i, chunk_text in enumerate(chunk_texts):\n",
    "        enriched_chunks.append({\n",
    "            \"text\": chunk_text,\n",
    "            \"embedding\": embeddings[i].tolist()     \n",
    "        })\n",
    "    \n",
    "    document['chunks'] = enriched_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.009157266467809677,\n",
       " -0.015870476141572,\n",
       " -0.01508508063852787,\n",
       " -0.04422919452190399,\n",
       " -0.12381163984537125,\n",
       " 0.009335299022495747,\n",
       " 0.04907502979040146,\n",
       " -0.058677442371845245,\n",
       " -0.05796046555042267,\n",
       " -0.0022972796577960253]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document['chunks'][0][\"embedding\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando corpus vectorizado en: ../output/corpus_vectorizado.json\n"
     ]
    }
   ],
   "source": [
    "output_path = \"../output/corpus_vectorizado.json\"\n",
    "\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "print(f\"Guardando corpus vectorizado en: {output_path}\")\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(corpus, f, indent=2, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
